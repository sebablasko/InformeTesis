\begin{conclusion}
Como se expuso en el presente trabajo, el inminente crecimiento de las redes de computadoras y el explosivo aumento en el uso de distintos dispositivos que demandan conectividad hacen que volver la Internet más robusta, disponible y eficiente sea una prioridad. Entendiendo como parte crucial de dicha labor el trabajo del servicio DNS, postular mejoras que optimicen su operación es un ejercicio plenamente justificado en pos de brindar una mejor performance sobre la red. Es tras ese afán --y apoyado en un fundamento teórico probado-- que se han realizado distintos esfuerzos por mejorar el rendimiento en el consumo de datos desde una interfaz lógica de red incorporando paralelismo, pero sin los resultados esperados. 
 
 
A lo largo de esta investigación se repasaron las principales sospechas vigentes que tratan de dar explicación al mal rendimiento presentado en el consumo concurrente de datos desde una interfaz de red, dividiendo el problema en 3 estudios: 
 
 
A través del \textbf{estudio de operación de las primitivas de sincronización del sistema} se corroboró que frente a condiciones de concurrencia, la actividad de sincronización dada por las llamadas a funciones de dicho tipo se incrementa a medida que se usan más hilos en el consumo de datos, volviéndose cada vez más significativa este tipo de funciones tanto en términos porcentuales como en términos del tiempo de ejecución total. Se determinó además que el protagonismo en dichas llamadas está asociado al control de bloqueo de spinlocks que protegen los sockets y que la tendencia en la duración de estas llamadas se ajusta de manera muy exacta a una curva de tipo logarítmico, algo interesante pues se rige por la misma tendencia que se determinó dominaba en el registro de tiempos netos de consumo de datos del caso de estudio, marcando una correlación directa entre ambos fenómenos y dando a entender que el control del spinlock termina impactando directamente en los tiempos totales de operación del socket. 
 
 
Con respecto al \textbf{estudio de canales de comunicación de hardware}, centrado en la dinámica de los \emph{performance counters} disponibles en el sistema, se logró corroborar el comportamiento entre componentes de hardware y cómo dicha dinámica se vuelve caótica a medida que se emplea una solución basada en concurrencia sobre un mismo socket, ello apoyado en el explosivo incremento de eventos como correcciones de caché, predicciones de procesamiento erróneas y rescate de datos de distintos niveles de memoria, entre otros. Más aún, este estudio demostró que el sistema --como un conjunto-- presenta un comportamiento explosivo en todas las aristas estudiadas, evidenciado en una alta correlación entre los eventos registrados al evaluarlos en un régimen concurrente. Nuevamente, se obtuvieron resultados que avalaban la sospecha de que la estructura socket no presenta un rendimiento escalable a medida que se incorporan operaciones concurrentes, reafirmando la hipótesis de que dicha estructura no tendría un diseño compatible con estrategias de consumo paralelo como las evaluadas. 
 
 
Por otro lado, nuestro \textbf{estudio de distribución de carga} usando \emph{processor affinity} demostró que aún cuando la arquitectura NUMA evaluada está optimizada para un mejor rendimiento basada en un procesamiento con datos distribuidos, las estrategias usadas para la asociación de hilos de ejecución a distintos cores no mostró ningún beneficio práctico. Más aún, se concluyó por una parte que al intentar aprovechar los distintos núcleos de procesamiento se cae en la misma situación detectada al usar concurrencia, ello pues los recursos terminan dispersándose entre los nodos NUMA y sus subestructuras de almacenamiento de datos, perdiendo el beneficio dado por el principio de localidad de memoria, y haciendo más dificil la coordinación en el acceso al socket mismo. Y por otro lado, que la aplicación de esquemas que reúnen los hilos en una misma componente de procesamiento termina por serialziar los accesos concurrentes, brindando un rendimiento equivalente a usar un sólo hilo. El comportamiento anterior concluye que las estratégias de procesamiento distribuido entre cores no brindan beneficio alguno pues a pesar de estar distribuyendo la carga de procesamiento se mantiene un mismo cuello de botella en el acceso por concepto del mecanismo de protección del socket compartido (nuevamente, el spinlock del socket). 
 
 
De esta forma, se pudo corroborar responsabilidades transversales y cruzadas entre las sospechas vigentes del problema estudiado. Por un lado transversales, pues todas las sospechas resultaron ser corroboradas experimentalmente, dando a entender que --en mayor o menor grado-- la operación de las interfaces de red de Linux no incorpora en su diseño una aplicación que admita concurrencia en el consumo de información, limitando así la capacidad de optimizaciones usando una estrategia de paralelismo a secas. Y por otro lado cruzadas, en el sentido de que los factores responsables de cada sospecha resultan repetirse entre cada caso, siendo el conjunto de variables que constituyen un Internet socket --en particular, los spinlock de protección del mismo-- elementos que al ser compartidos, sobrecargan los protocolos de coherencia y consistencia de datos al incurrir en estratégias concurrentes.
 
 
Como diagnóstico final reuniendo las principales conclusiones de los tres estudios contemplados en la presente investigación se concluye que, tal y como se postuló como hipótesis inicial, la estructura socket implementada a nivel del kernel de Linux no es apta para operar correctamente en condiciones de concurrencia, ello debido a que la misma no estaría diseñada para soportar dinámicas de acceso paralelo. Por otro lado, se concluye también que la arquitectura NUMA evaluada no puede sacar verdadero provecho del paralelismo sobre una estructura socket pues, por su diseño, los mecanismos de protección de spinlocks asociados a los sockets producen efectos de cuello de botella que terminan impactando negativamente al sistema al sobrecargar los mecanismos de coherencia y corrección de caché. En este escenario, resulta incompatible la aplicación de verdadera distribución de trabajo entre distintas componentes de procesamiento cuando se tiene un punto de contención detectado que fuerza una dinámica de serialización en el acceso al socket, entorpeciendo la coordinación de los distintos hilos que acceden al socket y terminando por degradar los tiempos netos de operación. 
 
 
Resulta interesante el que, al día de hoy, el problema estudiado sigue vigente y abierto a nuevas propuestas de soluciones. En esta misma línea, recientemente Facebook, a través de su equipo de desarrollo de kernel ha postulado modificaciones al núcleo de Linux que permitan hacer un uso más flexible en el consumo concurrente de datos desde estructuras protegidas por medio de estructuras denominadas \emph{Blk-mq} \cite{post:facebookFin}, que van en busca de aprovechar consumo distribuidos de colas usando lecturas exclusivas por cada CPU. Un enfoque que aprovecharía de mejor manera las interacciones de hardware del sistema al tener accesos más localizados que lo detectado en nuestro estudio de \emph{performance counters} e inspirado sobre la misma idea que nuestro estudio de \emph{processor affinity} pero implementado directamente a nivel del kernel. Por otro lado, una reciente publicacion de la revista \emph{;login;} de Usenix \cite{magazine:login} presentó un artículo sobre cómo aprovechar arquitecturas NUMA a modo de incrementar el rendimiento de operaciones sobre estructuras compartidas apoyándose en un estudio basado en performance counters, un enfoque muy similar al que se repasó en nuestra investigación combinando los enfoques de los estudios de \emph{performance counters} y \emph{processor affinity}, mostrando que las estrategias de análisis aplicadas en el presente trabajo están plenamente vigentes y siendo empleadas en investigaciones de alto nivel para sistemas y problemas actuales. 
 
 
En respuesta al problema, se postuló una solución basada en un módulo del kernel para Linux que, bajo un escenario de operación fiel al caso de estudio presentado, se presenta plenamente comparable con reuseport --la mejor opción disponible para paliar el problema estudiado en cuestión-- siendo competitivo en rendimiento con este último, brindando en la práctica un desempeño muy similar. La solución desarrollada cuenta con varios aciertos que la distinguen de reuseport: Una característica de balanceo de trabajo que ni el mismo reuseport incorpora, la que brinda una distribución de tiempos de trabajo más equitativa entre los sockets que trabajan conjuntamente, con respecto a reuseport. Por otro lado, ser una solución extensible y modificable, lo que garantiza su fácil adaptación para escenarios de operación específicos de distribución de paquetes entre sockets e incluso la capacidad de portar la solución misma a otras plataformas. Finalmente, el ser un desarrollo modular (Al ser un módulo del kernel) es una característica que permite postular la solución como una alternativa válida sobre cualquier entorno Linux y la hace invasiva con respecto al resto del sistema.  
 
 
Así también, esta investigación develó que el desempeño de las estrategias que involucran paralización no producen las ganancias de tiempo que se esperarían de dichas técnicas de procesamiento en el escenario estudiado --apoyando el diagnóstico de que los sockets no estarían diseñados para soportar dicha forma de operación--, lo cual hace pensar que, a menos de que se enfrente un escenario muy restrictivo que obligue a emplear técnicas de compartición de interfaces de red, o que ameriten mecanismos inteligentes de distribución de carga de datos, la alternativa de emplear un único socket resulta acertada y más que suficiente para obtener un buen rendimiento. 


Los resultados obtenidos con la presente investigación plantean distintas líneas de desarrollo que seguir como trabajo futuro. Un camino interesante corresponde al desarrollo de un modelo de costos basado en todas las variables recopiladas a lo largo de la presente investigación, el cual permita modelar la interacción de las distintas componentes estudiadas de manera de reconocer operaciones de alto costo computacional en la practica, y así poder optar a optimizaciones del sistema para lograr mejoras en los tiempos de ejecución. Un análisis de ésta naturaleza se puede hacer a partir del trabajo desarrollado en los estudios de operación de las primitivas de sincronización del sistema y de canales de comunicación de hardware desarrollados en esta tesis, los cuales ya proveen abundante información individual de las componentes del sistema frente a la ejecución de una tarea especifica. Otro camino en pos de profundizar esta investigación va de la mano del estudio de distribución de carga desarrollado en la misma, del cual se pueden evaluar mejores técnicas de distribución de trabajo según la arquitectura del sistema huésped y de los recursos disponibles, a fin de conseguir mejoras significativas al emplear estrategias de ejecución concurrente. Finalmente, una labor más a considerar como trabajo futuro es la relacionada con el desarrollo más acabado de la solución planteada en esta tesis, en el sentido de ampliar su operación diseñando y evaluando nuevos esquemas de distribución que se adapten mejor a distintos requerimientos de aplicaciones o sistemas de arquitecturas especificas.
\end{conclusion}